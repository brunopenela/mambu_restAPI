from argparse import ArgumentParser, ArgumentTypeError
import subprocess
from datetime import datetime as dt,timedelta
import sys
subprocess.check_call([sys.executable, '-m','pip','install','requests'])
import requests
import os
import json
parser= ArgumentParser()

parser.add_argument(
	"-i",
	"--input",
	help="URL for the request",
	required=True
)

parser.add_argument(
	"-o",
	"--output",
	help="Path for the folder where the output files will be generated",
	required=True
)

# parser.add_argument(
	# "-ap",
	# "--apikey",
	# help="Apikey Header for the http request to be made",
	# required=True
# )
# parser.add_argument(
	# "-au",
	# "--authorization",
	# help="Authorization header for the http request to be made",
	# required=True
# )
parser.add_argument(
	"-aut",
	"--authorization_token",
	help="Authorization header for the http request to be made",
	required=True
)
parser.add_argument(
	"-er",
	"--externalRef",
	help="ExteranlRef Header for the http request to be made",
	required=True
)
parser.add_argument(
	"-a",
	"--accept",
	help="Accept header for the http request to be made",
	required=True
)
parser.add_argument(
	"-dbef",
	"--datebefore",
	help="For incremental extraction before date to be considered as the start point",
	required=True
)
parser.add_argument(
	"-dft",
	"--dateafter",
	help="For incremental extraction final date to be considered as the end point",
	required=True
)
parser.add_argument(
	"-u",
	"--user",
	help="User name for the authentication to the HTTP request",
	required=True
)
parser.add_argument(
	"-p",
	"--password",
	help="Password for the authentication to the HTTP request",
	required=True
)
parser.add_argument(
	"-ct",
	"--contenttype",
	help="Content type header for the http request",
	required=True
)

parser.add_argument(
	"-ts",
	"--table_search",
	help="List of tables to be fetched by the Search version of the V2 API",
	required=False
)
parser.add_argument(
	"-tga",
	"--table_getall",
	help="List of tables to be fetched by the Get all version of the V2 API",
	required=False
)
parser.add_argument(
	"-tgad",
	"--table_getall_in_deposits",
	help="List of tables which the request requires a specific deposit account encodedkey to be included in the URL",
	required=False
)
parser.add_argument(
	"-tgac",
	"--table_getall_in_currencies",
	help="List of tables which the request requires a specific currency code to be included in the URL",
	required=False
)
parser.add_argument(
	"-tgacl",
	"--table_getall_in_clients",
	help="List of tables which the request requires a specific client encodedkey to be included in the URL",
	required=False
)
parser.add_argument(
	"-tgai",
	"--table_getall_in_indexratesources",
	help="List of tables which the request requires a specific index rate source encodedkey to be included in the URL",
	required=False
)
parser.add_argument(
	"-glt",
	"--glaccount_types",
	help="List of GLAccount types to be fetched (must also be included in the URL)",
	required=False
)
parser.add_argument(
	"-tgav",
	"--table_getall_in_v1",
	help="List of entities to be obtained through the V1 API",
	required=False
)
parser.add_argument(
	"-m",
	"--mode",
	help="Table extraction mode, acceptable values are: incremental; full; partial_full [table list (separated by blank space)]",
	required=False
)


args = parser.parse_args()

#Filter dictionary by which to filter the entities to be fetched by Search
filters = {"deposits/transactions" : "creationDate","gljournalentries":"creationDate","groups":"lastModifiedDate",
"deposits":"lastModifiedDate","clients":"lastModifiedDate","groups":"lastModifiedDate"}
# %%
INPUT_URL = str(args.input)
#INPUT_PATH THe base URL for the api call to get the REST entities


table_extraction_mode = str(args.mode)

#Checking if full snapshot
full_snapshot = table_extraction_mode == "full"

#Checking if any tables are to be extracted fully
full_snapshot_tables = []
if table_extraction_mode.split(" ")[0] == "partial_full":
    full_snapshot_tables += table_extraction_mode.split(" ")[1:]


table_list_search = args.table_search.split(" ")

table_list_getall = args.table_getall.split(" ")

table_list_getall_from_deposits = args.table_getall_in_deposits.split(" ")

table_list_getall_from_currencies = args.table_getall_in_currencies.split(" ")

table_list_getall_from_clients = args.table_getall_in_clients.split(" ")

table_list_getall_from_indexratesources = args.table_getall_in_indexratesources.split(" ")


glaccount_type_list = args.glaccount_types.split(" ")

table_list_getall_from_v1 = args.table_getall_in_v1.split(" ")

OUTPUT_PATH = str(args.output)
#OUTPUT_PATH for the directory to store the several json files to be generated by the requests

accept = str(args.accept)
#Part of the headers to be used for the api call, specifically the 'accept' header 
#ONLY APPLICABLE FOR THE V2 API

contentType = str(args.contenttype)

dateBefore = str(args.datebefore)

dateAfter = str(args.dateafter)

user = str(args.user)

password = str(args.password)

# apikey = str(args.apikey)
#Part of the headers to be used for the api call, specifically the 'apikey' header

# authorization = str(args.authorization)
#Part of the headers to be used for the api call, specifically the 'authorization' header, for the first request, with user and password

authorization_token = str(args.authorization_token)
#Part of the headers to be used for the api call, specifically the 'authorization' header


exteranlRef = str(args.externalRef)
#Part of the headers to be used for the api call, specifically the 'externalRef' header


# input_headers_first = {"accept":accept,"Authorization":authorization,"x-wu-externalRefId":exteranlRef,"Content-Type":contentType}


input_headers_second = {"accept":accept,"Authorization":authorization_token,"x-wu-externalRefId":exteranlRef,"Content-Type":contentType}
#Headers for the V2 request with user authentication

input_headers_v1 = {"Authorization":authorization_token,"x-wu-externalRefId":exteranlRef,"Content-Type":contentType}
#Headers for the V1 requests

#Can be defined anywhere from 0-1000 but 1000 seems to have better performance
limit = 1000

#Defining the date parameters to be included in the getAll requests (these have to go in the URL)
#%2B IS ENCODED AS +
#Assuming Berlin time
get_all_suffix_datetime = "&from=" + dateBefore + "T00:00:00%2B02:00" + "&to=" + dateAfter + "T00:00:00%2B02:00"
get_all_suffix_datetime_full_snapshot = "&from=1900-01-01T00:00:00%2B02:00" + "&to=2030-12-31T00:00:00%2B02:00"
get_all_suffix = "&from=" + dateBefore + "&to=" + dateAfter
get_all_suffix_full_snapshot = ""

#Gathering the list of entities which have to be specified in some request
account_list = []
currency_list = []
client_list = []
indexratesources_list = []
card_reference_list = []
user_list = []

#Defining, in order of priority, the date fields to which we filter incrementally by
date_fields = ['entryDate','creationDate','timestamp','lastModifiedDate']
found_date = False

#Path to write the files in
staged_path = "".join(OUTPUT_PATH)
#Check if staged file path exists and if not, create it
if not os.path.exists(staged_path):
    os.makedirs(staged_path)

#Checking when we go through any of these entities to store    
is_currency = False    
is_deposit = False
is_client = False
is_user = False

#To look for date fields within the responses
found_date = False

#Starting a session
s = requests.Session()

#Authenticating with given credentials
s.auth = (user,password)

#Body for the search request when a full_snapshot is made
full_snapshot_body = {
  "filterCriteria": [
    {
      "field": "creationDate",
      "operator": "AFTER",
      "value": "1000-10-10"
    }
  ]
}
timestampp = "2022-10-25T00:02:23+02:00"

print(dt.strptime(timestampp[:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(timestampp[-4])))
# def add_time(timestamp):
    # added_hours = "".join(timestamp[-5,-4])
    # added_mins = "".join(timestamp[-2:])
    # after_utc = timestamp[-6] == "+"
    



organization_timezone_tables = ["activities", "branch", "deposits", "loanaccount"]

#Iterating through all the search entities
for table in table_list_search:
    #Check if tables are a part of the full_snapshot
    if not full_snapshot and table not in full_snapshot_tables:
        #Looking for the following tables to list all encodedkeys
        is_deposit = table == "deposits"
        is_client = table == "clients"
        #Defining body to go with the post request for the tables that have a search API, varying with table
        body = {
          "filterCriteria": [
            {
              "field": filters[table],
              "operator": "BETWEEN",
              "value": dateBefore + "T00:00:00+02:00",
              "secondValue": dateAfter + "T00:00:00+02:00"
            }
          ]
        }
        #URL must include the entity name and :search
        URL = INPUT_URL + table + ":search"
        #Replacing forward slash to avoid errors when creating the file
        table = table.replace("/","_")
        #Starting at offset 0, meaning no records are skipped
        offset = 0
        #The first request must include paginationdetails=ON to know how many total records we need to go through
        URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
        r = s.post(url=URL+URL_SUFFIX,json = body,headers = input_headers_second,timeout =  30)
        #Checking the status
        print(r.status_code)
        writing = []
        #Opening the file with append state to keep writing if the total items exceed the limit in the first request
        f = open(staged_path + table + ".json", 'a')
        writing = json.loads(json.dumps(r.json()))
        #Write a json file for each request, separating the values by line
        if len(writing) > 0:
            #Looking for any date fields to filter by
            if not found_date:
                for date in date_fields:
                    if date in writing[0].keys():
                        found_date = date
                        print("Found date for table: "+table+", using date = " + date)
            if found_date:
                for x in writing:
                    x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                    f.write(''.join(json.dumps(x) + '\n'))
            else:
                print('No date field found for table '+table)
                f.write(''.join([json.dumps(x) + '\n' for x in writing]))
        #Once the total items are obtained, pagination details are turned off for performance reasons
        URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
        if is_deposit:
            #Listing all accounts that were changed
            account_list += [x['encodedKey'] for x in writing]
        if is_client:
            #Listing all clients that were changed
            client_list += [x['encodedKey'] for x in writing]
        if "items-total" in [x.lower() for x in r.headers.keys()]:
            total_entries = int(r.headers.get("items-total"))
            #While not all records have been loaded, keep increasing offset until it reaches the total
            while limit + offset < total_entries:
                #Increase the offset by the value of limit
                offset+= limit
                URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                #One additional request by page of responses is necessary
                r = s.post(url=URL+URL_SUFFIX,json = body,headers = input_headers_second,timeout =  30)
                writing = json.loads(json.dumps(r.json()))
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                else:
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                if is_deposit:
                    account_list += [x['encodedKey'] for x in writing]
                if is_client:
                    client_list += [x['encodedKey'] for x in writing]
        found_date = False
    else:
        is_deposit = table == "deposits"
        is_client = table == "clients"
        #Changing the body to fit the full_snapshot criteria
        body = full_snapshot_body
        URL = INPUT_URL + table + ":search"
        #Replacing forward slash to avoid errors when creating the file
        table = table.replace("/","_")
        #Starting at offset 0, meaning no records are skipped
        offset = 0
        #The first request must include paginationdetails=ON to know how many total records we need to go through
        URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
        r = s.post(url=URL+URL_SUFFIX,json = body,headers = input_headers_second,timeout =  30)
        #Checking the status
        print(r.status_code)
        writing = []
        #Opening the file with append state to keep writing if the total items exceed the limit in the first request
        f = open(staged_path + table + ".json", 'a')
        writing = json.loads(json.dumps(r.json()))
        if len(writing) > 0:
            #Looking for any date fields to filter by
            if not found_date:
                for date in date_fields:
                    if date in writing[0].keys():
                        found_date = date
                        print("Found date for table: "+table+", using date = " + date)
            if found_date:
                for x in writing:
                    x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                    f.write(''.join(json.dumps(x) + '\n'))
            #Write a json file for each request, separating the values by line
            else:
                print('No date field found for table '+table)
                f.write(''.join([json.dumps(x) + '\n' for x in writing]))
        #Once the total items are obtained, pagination details are turned off for performance reasons
        URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
        if is_deposit:
            #Listing all accounts
            account_list += [x['encodedKey'] for x in writing]
        if is_client:
            #Listing all clients
            client_list += [x['encodedKey'] for x in writing]
        if "items-total" in [x.lower() for x in r.headers.keys()]:
            total_entries = int(r.headers.get("items-total"))
            #While not all records have been loaded, keep increasing offset until it reaches the total
            while limit + offset < total_entries:
                #Increase the offset by the value of limit
                offset+= limit
                URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                #One additional request by page of responses is necessary
                r = s.post(url=URL+URL_SUFFIX,json = body,headers = input_headers_second,timeout =  30)
                writing = json.loads(json.dumps(r.json()))
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                #Write a json file for each request, separating the values by line
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                if is_deposit:
                    account_list += [x['encodedKey'] for x in writing]
                if is_client:
                    client_list += [x['encodedKey'] for x in writing]
        found_date = False
print(len(account_list))

#Getting all the extraction from the get all APIs
for table in table_list_getall:
    #Check if tables are a part of the full_snapshot
    if not full_snapshot and table not in full_snapshot_tables:
        #Checking if glaccounts, because the request must specify the type
        if table == "glaccounts":
            for gltype in glaccount_type_list:
                URL = INPUT_URL + table
                #Replacing forward slash to avoid errors when creating the file
                table = table.replace("/","_")
                offset = 0
                #Including the type in the URL, as needed
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON&type=" + gltype
                #Get request instead of POST as is the case for the Search version
                r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
                print(r.status_code)
                writing = []
                #Opening the file with append state to keep writing if the total items exceed the limit in the first request
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                #Checking if the response has any values
                if len(writing) > 0:
                    #Looking for any date fields to filter by
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        #If no date fields is found then log all responses normally
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                #Turning off pagination details
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                #Some requests, despite having paginationdetails=ON don't include the "items-total" header, as susch, this must be checked
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_second,timeout =  30)
                        writing = json.loads(json.dumps(r.json()))
                        if found_date:
                            for x in writing:
                                if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                    #Only writing to the file if the found date is between the dates specified
                                    x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                    f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
        else:
            #Checking for the following tables
            is_user = table == 'users'
            is_currency = table == 'currencies'
            is_indexrate = table == "indexratesources"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = json.loads(json.dumps(r.json()))
            if len(writing) > 0:
                if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                            #Only writing to the file if the found date is between the dates specified
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if is_currency:
                #Logging the currency code for all currencies
                currency_list += [x['code'] for x in writing]
            if is_user:
                #Logging the usernames for the users changed
                user_list += [x['username'] for x in writing]
            if is_indexrate:
                #Logging the indexratesources encoded keys
                indexratesources_list += [x['encodedKey'] for x in writing]
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_second,timeout =  30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                    if is_currency:
                        currency_list += [x['code'] for x in writing]
                    if is_user:
                        user_list += [x['username'] for x in writing]
            found_date = False
    else:
        #No date filtering is applied in this case
        if table == "glaccounts":
            for gltype in glaccount_type_list:
                URL = INPUT_URL + table
                #Replacing forward slash to avoid errors when creating the file
                table = table.replace("/","_")
                offset = 0
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON&type=" + gltype
                #Change the url suffix to match the full snapshot
                r = s.get(url=URL+URL_SUFFIX + get_all_suffix_full_snapshot,headers = input_headers_second,timeout =  30)
                print(r.status_code)
                writing = []
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                if len(writing) > 0:
                    #Looking for any date fields to filter by
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    #Write a json file for each request, separating the values by line
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        #Change the url suffix to match the full snapshot
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+URL_SUFFIX+ get_all_suffix_full_snapshot,headers = input_headers_second,timeout =  30)
                        writing = json.loads(json.dumps(r.json()))
                        if found_date:
                            for x in writing:
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
        else:
            is_user = table == 'users'
            is_currency = table == 'currencies'
            is_indexrate = table == "indexratesources"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix_full_snapshot,headers = input_headers_second,timeout =  30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = json.loads(json.dumps(r.json()))
            if len(writing) > 0:
                #Looking for any date fields to filter by
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if is_currency:
                currency_list += [x['code'] for x in writing]
            if is_user:
                user_list += [x['username'] for x in writing]
            if is_indexrate:
                indexratesources_list += [x['encodedKey'] for x in writing]
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix_full_snapshot,headers = input_headers_second,timeout =  30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
                    if is_currency:
                        currency_list += [x['code'] for x in writing]
                    if is_user:
                        user_list += [x['username'] for x in writing]
            found_date = False
print(len(currency_list))
found_date = False
for table in table_list_getall_from_v1:
    if not full_snapshot and table not in full_snapshot_tables:
        if table == 'transactionchannels':
            v1_suffix = "&from=" + dateBefore + "T00:00:00+02:00" + "&to=" + dateAfter + "T00:00:00+02:00"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=BASIC&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_v1,timeout = 30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            #Delete the associated custom fields, which number 30k+ for a single transaction channel, without useful information
            for x in writing:
                del x['customFields']
            if len(writing) > 0:
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=BASIC&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_v1,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
        else:
            v1_suffix = "&from=" + dateBefore + "T00:00:00+02:00" + "&to=" + dateAfter + "T00:00:00+02:00"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_v1,timeout = 30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            if len(writing) > 0:
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                            #Only writing to the file if the found date is between the dates specified
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_v1,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
    else:
        if table == 'transactionchannels':
            v1_suffix = "&from=" + dateBefore + "T00:00:00+02:00" + "&to=" + dateAfter + "T00:00:00+02:00"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=BASIC&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_v1,timeout = 30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            for x in writing:
                del x['customFields']
            if len(writing) > 0:
               #Looking for any date fields to filter by
               if not found_date:
                   for date in date_fields:
                       if date in writing[0].keys():
                           found_date = date
                           print("Found date for table: "+table+", using date = " + date)
               if found_date:
                   for x in writing:
                       x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                       f.write(''.join(json.dumps(x) + '\n'))
               else:
                   print('No date field found for table '+table)
                   f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=BASIC&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_v1,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            found_date = False
        else:
            v1_suffix = "&from=" + dateBefore + "T00:00:00+02:00" + "&to=" + dateAfter + "T00:00:00+02:00"
            URL = INPUT_URL + table
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL+URL_SUFFIX + get_all_suffix,headers = input_headers_v1,timeout = 30)
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            if len(writing) > 0:
                #Looking for any date fields to filter by
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_v1,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            found_date = False
found_date = False
for table in table_list_getall_from_deposits:
    if not full_snapshot and table not in full_snapshot_tables:
        table = table.replace("/","_")
        URL = INPUT_URL
        if table == 'cards':
            for account in account_list:
                #Replacing forward slash to avoid errors when creating the file
                offset = 0
                DEPOSIT_SUFFIX = 'deposits/'+account+'/'
                DEPOSIT_SUFFIX += table
                #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
                r = s.get(url=URL + DEPOSIT_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
                #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
                print(r.status_code)
                if r.status_code != 200:
                    continue
                writing = []
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                for x in writing:
                    x['accountKey'] = account
                if len(writing) > 0:
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                card_reference_list += [x['referenceToken'] for x in writing]
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_second,timeout = 30)
                        writing = json.loads(json.dumps(r.json()))
                        card_reference_list += [x['referenceToken'] for x in writing]
                        if found_date:
                           for x in writing:
                               if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                   #Only writing to the file if the found date is between the dates specified
                                   x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                   f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False

        else:
            for account in account_list:
                #Replacing forward slash to avoid errors when creating the file
                offset = 0
                DEPOSIT_SUFFIX = 'deposits/'+account+'/'
                DEPOSIT_SUFFIX += table
                #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
                r = s.get(url=URL + DEPOSIT_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout = 30)
                #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
                print(r.status_code)
                if r.status_code != 200:
                    continue
                writing = []
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                if len(writing) > 0:
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                #Only writing to the file if the found date is between the dates specified
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                for x in writing:
                    x['accountKey'] = account
                f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_second,timeout = 30)
                        writing = json.loads(json.dumps(r.json()))
                        if found_date:
                            for x in writing:
                                if dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") > dt.strptime(dateBefore + " 00:00:00","%Y-%m-%d %H:%M:%S") and dt.strptime(x[found_date][:19],"%Y-%m-%dT%H:%M:%S") < dt.strptime(dateAfter + " 00:00:00","%Y-%m-%d %H:%M:%S"):
                                    #Only writing to the file if the found date is between the dates specified
                                    x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                    f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
    else:
        table = table.replace("/","_")
        URL = INPUT_URL
        if table == 'cards':
            for account in account_list:
                #Replacing forward slash to avoid errors when creating the file
                offset = 0
                DEPOSIT_SUFFIX = 'deposits/'+account+'/'
                DEPOSIT_SUFFIX += table
                #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
                r = s.get(url=URL + DEPOSIT_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
                #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
                print(r.status_code)
                if r.status_code != 200:
                    continue
                writing = []
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                for x in writing:
                    x['accountKey'] = account
                if len(writing) > 0:
                    #Looking for any date fields to filter by
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                #Storing the card token which is used for separate requests
                card_reference_list += [x['referenceToken'] for x in writing]
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+URL_SUFFIX+ get_all_suffix,headers = input_headers_second,timeout = 30)
                        writing = json.loads(json.dumps(r.json()))
                        card_reference_list += [x['referenceToken'] for x in writing]
                        if found_date:
                            for x in writing:
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                found_date = False
        else:
            for account in account_list:
                #Replacing forward slash to avoid errors when creating the file
                offset = 0
                DEPOSIT_SUFFIX = 'deposits/'+account+'/'
                DEPOSIT_SUFFIX += table
                #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
                URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
                r = s.get(url=URL + DEPOSIT_SUFFIX + URL_SUFFIX + get_all_suffix_full_snapshot,headers = input_headers_second,timeout = 30)
                #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
                print(r.status_code)
                #Sometimes the specified account doesn't have these associated entities and an error is returned, to avoid this we ignore any status != 200
                # if r.status_code != 200:
                    # continue
                writing = []
                f = open(staged_path + table + ".json", 'a')
                writing = json.loads(json.dumps(r.json()))
                for x in writing:
                    x['accountKey'] = account
                if len(writing) > 0:
                    #Looking for any date fields to filter by
                    if not found_date:
                        for date in date_fields:
                            if date in writing[0].keys():
                                found_date = date
                                print("Found date for table: "+table+", using date = " + date)
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
                URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
                if "items-total" in [x.lower() for x in r.headers.keys()]:
                    total_entries = int(r.headers.get("items-total"))
                    while limit + offset < total_entries:
                        offset+= limit
                        URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                        r = s.get(url=URL+DEPOSIT_SUFFIX+URL_SUFFIX+ get_all_suffix_full_snapshot,headers = input_headers_second,timeout = 30)
                        writing = json.loads(json.dumps(r.json()))
                        if found_date:
                            for x in writing:
                                x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                                f.write(''.join(json.dumps(x) + '\n'))
                        else:
                            print('No date field found for table '+table)
                            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            found_date = False
                        
                    
for table in table_list_getall_from_currencies:
    if not full_snapshot and table not in full_snapshot_tables:
        for currency in currency_list:
            URL = INPUT_URL
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            CURRENCY_SUFFIX = 'currencies/'+currency+'/'
            CURRENCY_SUFFIX += table
            #For fields that need a specific currency code, with the list of all currencies for which to get the rates from
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + CURRENCY_SUFFIX + URL_SUFFIX + get_all_suffix_datetime,headers = input_headers_second,timeout = 30)
            #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/currencies/RON/accountingRates
            print(r.status_code)
            #If the specified currency doesn't have rates, as is the case for the base currency, ignore the error response
            if r.status_code == 400:
                print(URL + CURRENCY_SUFFIX + URL_SUFFIX + get_all_suffix_datetime)
                print(r.text)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = json.loads(json.dumps(r.json()))
            if len(writing) > 0:
                #Looking for any date fields to filter by
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix_datetime,headers = input_headers_second,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
        found_date = False
    else:
        for currency in currency_list:
            URL = INPUT_URL
            #Replacing forward slash to avoid errors when creating the file
            table = table.replace("/","_")
            offset = 0
            CURRENCY_SUFFIX = 'currencies/'+currency+'/'
            CURRENCY_SUFFIX += table
            #For fields that need a specific currency code, with the list of all currencies for which to get the rates from
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + CURRENCY_SUFFIX + URL_SUFFIX + get_all_suffix_datetime_full_snapshot,headers = input_headers_second,timeout = 30)
            #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/currencies/RON/accountingRates
            print(r.status_code)
            #If the specified currency doesn't have rates, as is the case for the base currency, ignore the error response
            if r.status_code == 400:
                print(URL + CURRENCY_SUFFIX + URL_SUFFIX + get_all_suffix_datetime_full_snapshot)
                print(r.text)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = json.loads(json.dumps(r.json()))
            f.write(''.join([json.dumps(x) + '\n' for x in writing]))
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix_datetime_full_snapshot,headers = input_headers_second,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
print(currency_list)
#Looks wrong, look into
for table in table_list_getall_from_clients:
    if not full_snapshot and table not in full_snapshot_tables:
        table = table.replace("/","_")
        URL = INPUT_URL
        for client in client_list:
            #Replacing forward slash to avoid errors when creating the file
            offset = 0
            CLIENT_SUFFIX = 'clients/'+client+'/' + table.replace("/","_")
            #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + CLIENT_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout = 30)
            #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            if len(writing) > 0:
                #Looking for any date fields to filter by
                f.write('' + str(writing) + '\n') 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL+URL_SUFFIX+ get_all_suffix_datetime,headers = input_headers_second,timeout = 30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
        found_date = False
    else:
        table = table.replace("/","_")
        URL = INPUT_URL
        for client in client_list:
            #Replacing forward slash to avoid errors when creating the file
            offset = 0
            CLIENT_SUFFIX = 'clients/'+client+'/' + table.replace("/","_")
            #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + CLIENT_SUFFIX + URL_SUFFIX + get_all_suffix_full_snapshot,headers = input_headers_second,timeout = 30)
            #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = [r.json()]
            if len(writing) > 0:
                f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL + CLIENT_SUFFIX + URL_SUFFIX + get_all_suffix_full_snapshot,headers = input_headers_second,timeout = 30)
                    writing = r.json()
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                    
for table in table_list_getall_from_indexratesources:
    if not full_snapshot and table not in full_snapshot_tables:
        table = table.replace("/","_")
        URL = INPUT_URL
        for indexratesource in indexratesources_list:
            # Replacing forward slash to avoid errors when creating the file
            offset = 0
            INDEXRATESOURCES_SUFFIX = 'indexratesources/'+indexratesource+'/' + table.replace("/","_")
            #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + INDEXRATESOURCES_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
            # URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = json.loads(json.dumps(r.json()))
            if len(writing) > 0:
                #Looking for any date fields to filter by
                if not found_date:
                    for date in date_fields:
                        if date in writing[0].keys():
                            found_date = date
                            print("Found date for table: "+table+", using date = " + date)
                if found_date:
                    for x in writing:
                        x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                        f.write(''.join(json.dumps(x) + '\n'))
                else:
                    print('No date field found for table '+table)
                    f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL + INDEXRATESOURCES_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
                    writing = json.loads(json.dumps(r.json()))
                    if found_date:
                        for x in writing:
                            x[found_date] = str(dt.strptime(x[found_date][:19], '%Y-%m-%dT%H:%M:%S') - timedelta(hours = int(x[found_date][-4])))
                            f.write(''.join(json.dumps(x) + '\n'))
                    else:
                        print('No date field found for table '+table)
                        f.write(''.join([json.dumps(x) + '\n' for x in writing]))
        found_date = False
    else:
        table = table.replace("/","_")
        URL = INPUT_URL
        for indexratesource in indexratesources_list:
            #Replacing forward slash to avoid errors when creating the file
            offset = 0
            INDEXRATESOURCES_SUFFIX = 'indexratesources/'+indexratesource+'/' + table.replace("/","_")
            #For fields that need a specific account ID, with the list of all accounts that were changed in the interval
            URL_SUFFIX = "?limit="+str(limit)+"&offset=0&detailsLevel=FULL&paginationDetails=ON"
            r = s.get(url=URL + INDEXRATESOURCES_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
            #URL Example: https://admin-dev.consbank.mambu.partner.eu.cloud.wuintranet.net/api/deposits/8a9422297f030456017f08ff7fb302d7/authorizationholds
            print(r.status_code)
            writing = []
            f = open(staged_path + table + ".json", 'a')
            writing = r.json()
            if len(writing) > 0:
                f.write(''.join([json.dumps(x) + '\n' for x in writing])) 
            URL_SUFFIX.replace("paginationDetails=ON","paginationDetails=OFF")
            if "items-total" in [x.lower() for x in r.headers.keys()]:
                total_entries = int(r.headers.get("items-total"))
                while limit + offset < total_entries:
                    offset+= limit
                    URL_SUFFIX = "?limit="+str(limit)+"&offset="+str(offset)+"&detailsLevel=FULL&paginationDetails=OFF"
                    r = s.get(url=URL + INDEXRATESOURCES_SUFFIX + URL_SUFFIX + get_all_suffix,headers = input_headers_second,timeout =  30)
                    writing = json.loads(json.dumps(r.json()))
                    f.write(''.join([json.dumps(x) + '\n' for x in writing]))
                
